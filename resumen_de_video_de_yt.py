# -*- coding: utf-8 -*-
"""Resumen_de_video_de_yt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gsSoxriSEcXk4UoLhChGJmOBIdDc9j3K
"""

from google.colab import drive
import os
drive.mount('/content/gdrive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/MyDrive/Prueba IA

!pip install git+https://github.com/openai/whisper.git
!pip install pytube
!sudo apt-get remove -y youtube-dl

"""Parte del codigo donde se descarga el video de youtube"""

import os
from pytube import YouTube

#Cambia el link que quieras
LINK = "https://www.youtube.com/watch?v=I9LTxZoAxIY"

# Obtener objeto YouTube
yt = YouTube(LINK)

# Obtener objeto Stream de audio
audio_stream = yt.streams.filter(only_audio=True).first()

# Descargar archivo de audio en una ubicación temporal
temp_file_path = audio_stream.download()

# Renombrar archivo a "audio.mp3" y mover a la carpeta actual
os.rename(temp_file_path, os.path.join(os.getcwd(), "audio.mp3"))

import whisper
model = whisper.load_model("base")
result = model.transcribe("audio.mp3")

nombre_archivo = "texto_original.txt"

with open(nombre_archivo, "w") as archivo:
    archivo.write(result["text"])

# Cerrar el archivo
archivo.close()

"""Parte del codigo donde se genera el resumen"""

!pip install transformers

import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from transformers import BartTokenizer, BartForConditionalGeneration
import re

tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)

import math

def formateo_texto(texto):
    """
    Divide un texto en oraciones utilizando un patrón específico.

    Parámetros:
    - texto (str): El texto a formatear y dividir en oraciones.

    Retorna:
    list: Una lista de las oraciones resultantes después de dividir el texto.

    Ejemplo:
    >>> texto = "Hola. Esto es una prueba. Es un ejemplo. \n\nAdiós."
    >>> resultado = formateo_texto(texto)
    >>> print(resultado)
    ['Hola', 'Esto es una prueba', 'Es un ejemplo', 'Adiós.']
    """

    patron = r'\.\s+|\.(?=[A-Z])|\.\n|^\n+'
    return re.split(patron, texto)

def cortar_cadena(cadena, partes):
    longitud = len(cadena)
    tamano_parte = longitud // partes
    partes_cortadas = [cadena[i:i+tamano_parte] for i in range(0, longitud, tamano_parte)]
    return partes_cortadas

# Cargar el tokenizador y el modelo pre-entrenado

def contar_tokens_oraciones(lista_oraciones):
    """
    Cuenta el número de tokens en cada oración de una lista y devuelve una lista de oraciones con sus respectivos números de tokens, junto con el total de tokens.

    Parámetros:
    - lista_oraciones (list): Una lista de oraciones.

    Retorna:
    tuple: Una tupla que contiene dos elementos:
        - lista_oraciones_tokens (list): Una lista de sublistas donde cada sublista contiene una oración y su número de tokens.
        - tokens_totales (int): El número total de tokens en todas las oraciones.

    Ejemplo:
    >>> lista_subcadenas = ['Hola', 'Esto es una prueba', 'Es un ejemplo', 'Adiós.']
    >>> resultado_oraciones, total_tokens = contar_tokens_oraciones(lista_subcadenas)
    >>> print(resultado_oraciones)
    [['Hola', 1], ['Esto es una prueba', 5], ['Es un ejemplo', 4], ['Adiós.', 2]]
    >>> print(total_tokens)
    12
    """

    lista_oraciones_tokens = []
    tokens_totales = 0

    for oracion in lista_oraciones:
        aux = []
        aux2 = len(tokenizer.encode(oracion, return_tensors='pt')[0])
        if aux2 > 1024:
          cant=math.ceil(aux2/1024)
          cadenas=cortar_cadena(oracion, cant)
          for i in cadenas:
            aux = []
            aux2=len(tokenizer.encode(i, return_tensors='pt')[0])
            aux.append(i)
            aux.append(aux2)
            lista_oraciones_tokens.append(aux)
            tokens_totales += aux2
        else:
          aux.append(oracion)
          aux.append(aux2)
          lista_oraciones_tokens.append(aux)
          tokens_totales += aux2
    return lista_oraciones_tokens, tokens_totales

def resumo(texto):
    """
    Genera un resumen a partir de un texto utilizando un modelo de generación de lenguaje.

    Parámetros:
    - texto (str): El texto del cual se desea generar un resumen.

    Retorna:
    str: El resumen generado a partir del texto.

    Ejemplo:
    >>> texto = "Este es un ejemplo de texto largo que se desea resumir utilizando un modelo de generación de lenguaje."
    >>> resultado = resumo(texto)
    >>> print(resultado)
    "Este es un resumen generado por el modelo de generación de lenguaje."

    Nota:
    - El resumen se genera utilizando un modelo pre-entrenado `model` y un tokenizer `tokenizer`.
    - La longitud máxima del resumen generado se puede ajustar modificando el valor de `max_length`.
    - Los tokens especiales se omiten al decodificar el resumen utilizando `skip_special_tokens=True`.
    """

    tokens = tokenizer.encode(texto, truncation=True, max_length=1024, return_tensors='pt').to(device)
    
    max_length = 512  # Definir la longitud máxima deseada para la generación
    resumen_codificado = model.generate(tokens, max_length=max_length).to(device)
    return tokenizer.decode(resumen_codificado[0], skip_special_tokens=True)



def creo_resumen(lista_oraciones):
    """
    Genera un resumen a partir de una lista de oraciones.

    Parámetros:
    - lista_oraciones (list): Una lista de oraciones a partir de las cuales se generará el resumen.

    Retorna:
    str: El resumen generado a partir de las oraciones.

    Ejemplo:
    >>> lista_oraciones = ["Esta es una oración.", "Esta es otra oración.", "Y esta es una tercera oración."]
    >>> resultado = creo_resumen(lista_oraciones)
    >>> print(resultado)
    "Este es un resumen generado a partir de las oraciones dadas."

    Nota:
    - El resumen se genera utilizando las funciones auxiliares `formateo_texto`, `contar_tokens_oraciones` y `resumo`.
    - El margen de redundancia se determina mediante el valor de `redu`, donde 0 indica un margen mínimo de redundancia y 0.99 indica un margen máximo de redundancia.
    - La cantidad máxima de tokens por vez se establece mediante el valor de `cant_datos`, donde 0 indica que no se permiten tokens y 1024 indica el máximo posible según el modelo utilizado.
    - Las oraciones se concatenan en el resumen hasta que se alcanza el margen de redundancia o la cantidad máxima de tokens.
    - El resumen generado es una cadena de texto que se va construyendo en cada iteración del bucle.
    """

    redu = 0.9  # El margen de redundancia
    cant_datos = 1024  # Cantidad máxima de tokens por vez
    cant_resumem = 0
    resumen = ''
    text = ''
    largo_array = 0
    sum = 0
    cantidad_bucles=0
    while cant_resumem != 1 and cantidad_bucles <10:
        cantidad_bucles+=1
        print('-------------------------------------------')
        print(f'cantidad de bucles: {cantidad_bucles}')
        print('-------------------------------------------')
        inicio = i = indice_redu = 0
        cant_resumem = 0
        if resumen != '':
            lista_oraciones = formateo_texto(resumen)
            lista_oraciones, aux = contar_tokens_oraciones(lista_oraciones)
        largo_array = len(lista_oraciones)
        print(f'largo array: {largo_array }')
        while i < largo_array:
            sum += lista_oraciones[i][1]
            print(f'suma parcial: {sum}')
            print(f'indice: {i}')
            if indice_redu == inicio and sum > cant_datos * redu:
                indice_redu = i
            if sum < cant_datos:
                text += lista_oraciones[i][0] + '. '
                i += 1
            else:
                sum = 0
                cant_resumem += 1
                resumen += resumo(text) + ' '
                text = ''
                i = inicio = indice_redu
        if resumen == '':
            resumen = resumo(text)
            cant_resumem += 1
        print('----------------------------------------------------------')
        print(f'cantidad de resumenes -> {cant_resumem} ')
        print('----------------------------------------------------------')        
    return resumen

texto_formateado= formateo_texto(result["text"])
lista_oraciones,_= contar_tokens_oraciones(texto_formateado)
resumen_final=creo_resumen(lista_oraciones)
print(resumen_final)